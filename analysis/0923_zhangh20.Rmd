---
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
# Required R package installation:
# These will install packages if they are not already installed
# Set the correct default repository
r = getOption("repos")
r["CRAN"] = "http://cran.rstudio.com"
options(repos = r)


if (!require("knitr")) {
  install.packages("knitr")
  library(knitr)
}

if (!require("kableExtra")) {
  install.packages("kableExtra")
  library(kableExtra)
}

knitr::opts_chunk$set(echo = TRUE)

source("Elasticsearch.R")
source("plot_tweet_sentiment_timeseries.R")
```

### Configure the search parameters here:

```{r}
# query start date/time (inclusive)
rangestart <- "2020-03-01 00:00:00"

# query end date/time (exclusive)
rangeend <- "2020-08-01 00:00:00"

# text filter restricts results to only those containing words, phrases, or meeting a boolean condition. This query syntax is very flexible and supports a wide variety of filter scenarios:
# words: text_filter <- "cdc nih who"  ...contains "cdc" or "nih" or "who"
# phrase: text_filter <- '"vitamin c"' ...contains exact phrase "vitamin c"
# boolean condition: <- '(cdc nih who) +"vitamin c"' ...contains ("cdc" or "nih" or "who") and exact phrase "vitamin c"
#full specification here: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html
text_filter <- ""

# location filter acts like text filter except applied to the location of the tweet instead of its text body.
location_filter <- ""

# if FALSE, location filter considers both user-povided and geotagged locations. If TRUE, only geotagged locations are considered.
must_have_geo <- TRUE 

# query semantic similarity phrase
semantic_phrase <- ""

# return results in chronological order or as a random sample within the range
# (ignored if semantic_phrase is not blank)
random_sample <- TRUE
# if using random sampling, optionally specify a seed for reproducibility. For no seed, set to NA.
random_seed <- NA
# number of results to return (to return all results, set to NA)
resultsize <- 10000
# minimum number of results to return. This should be set according to the needs of the analysis (i.e. enough samples for statistical significance)
min_results <- 1
```

### Results:

```{r, echo=FALSE}
results <- do_search(indexname="coronavirus-data-masks",  #coronavirus-data-masks
                     rangestart=rangestart,
                     rangeend=rangeend,
                     text_filter=text_filter,
                     location_filter=location_filter,
                     semantic_phrase=semantic_phrase,
                     must_have_geo=must_have_geo,
                     random_sample=random_sample,
                     random_seed=random_seed,
                     resultsize=resultsize,
                     resultfields='"created_at", "user.screen_name", "user.location", "normalized_state","place.full_name", "place.country", "text", "full_text", "extended_tweet.full_text", "sentiment.vader.primary"',
                     elasticsearch_host="lp01.idea.rpi.edu",
                     elasticsearch_path="elasticsearch",
                     elasticsearch_port=443,
                     elasticsearch_schema="https")

required_fields <- c("created_at", "user_screen_name", "user_location","normalized_state", "place.full_name", "place.country", "full_text", "sentiment.vader.primary")
validate_results(results$df, min_results, required_fields)

```

### Print:
```{r, echo=FALSE}
#Transform results for sentiment plot
results.df <- results$df
colnames(results.df)[colnames(results.df) == "sentiment.vader.primary"] <- "sentiment"
results.df$vector_type <- "tweet"

#Transform results for tweet display
display.df <- results.df
display.df$user_location <- ifelse(is.na(display.df$place.full_name), display.df$user_location, paste(display.df$place.full_name, display.df$place.country, sep=", "))
display.df$user_location[is.na(display.df$user_location)] <- ""
display.df$user_location_type <- ifelse(is.na(display.df$place.full_name), "User", "Place")
display_fields <- c("full_text", "created_at", "user_screen_name", "user_location", "user_location_type", "sentiment")
if (semantic_phrase != "") {
  display_fields <- c("cosine_similarity", display_fields)
}
display.df <- display.df[,display_fields]

#print results
params.df <- data.frame(from=results$params$rangestart, 
                        to=results$params$rangeend,
                        text.filter=results$params$text_filter,
                        location.filter=results$params$location_filter,
                        phrase=results$params$semantic_phrase,
                        geo_only=results$params$must_have_geo,
                        results.count=paste(nrow(results$df), "/", results$total))
kable(params.df) %>% kable_styling()
```
### Search: 
```{r,echo=FALSE}
'
print(results.df[results.df$place.country=="United States",])
'
```

###Some Functions: 
```{r,echo=FALSE}
find.State <- function(row){
  usrPL <- row$place.full_name
  abb <- state.abb 
  name <- state.name 
  for( itr in abb )
  {
    if( grepl(itr, usrPL, fixed=TRUE))
    {
      return(itr)
    }
  }
  for(idx in 1:length(abb))
  {
    itr = name[idx]
    if(grepl(itr,usrPL,fixed=TRUE)){
      return(abb[idx])
    }
  }
  return("USA")
}

checkNormalized<-function(row){
  return(is.nan(row$normalized_state))
}

```

```{r,echo =FALSE}
#results.df[is.na(results.df$normalized_state)=TRUE,]
```


### Check:
```{r,echo=FALSE}
tweets.df <- results.df[is.na(results.df$normalized_state)!= TRUE,]
tweets.df$created_at <- as.POSIXct(strptime(tweets.df$created_at, format="%a %b %d %H:%M:%S +0000 %Y", tz="UTC"))
tweets.df$week <- epiweek(tweets.df$created_at)  # find CDC epidemiological week
tweets.df$date <- date(tweets.df$created_at)
tweet.tibble <- tibble(sentiment = tweets.df$sentiment, week = tweets.df$week, date = tweets.df$date, datetime = tweets.df$created_at, location = tweets.df$normalized_state  )

summary.tibble <- tweet.tibble %>% group_by(location) %>% summarize(mean_sentiment = mean(sentiment), sd_sentiment = sd(sentiment), count = length(datetime), divisiveness = divisiveness_score(sentiment))
summary.tibble$divisiveness[is.na(summary.tibble$divisiveness)] <- 0

summary.tibble <- summary.tibble %>% ungroup()

fig1 <- ggplot(summary.tibble, aes(x = location, y = count, fill = mean_sentiment)) + 
      geom_bar(stat = "identity", color = "azure3") + 
      scale_fill_gradient2(name = "Sentiment Average", limits = c(-1,1), low = "red", mid = "white", high = "green", midpoint = 0) +
      ggtitle("Tweets by Location") + 
      ylab("Tweet Count") +
      theme(axis.title.x = element_blank() ,axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)   )
fig2 <- ggplot(summary.tibble, aes(x = location, y = divisiveness)) + 
      geom_bar(fill = "purple", stat = "identity") + 
      geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
      ylab("Divisiveness") +
      xlab("Date") + 
      theme_grey(base_size = 9) + theme(axis.text.x=element_text(angle=90))
fig3<- ggplot(summary.tibble, aes(x = location, y =  mean_sentiment, fill = mean_sentiment)) + 
       geom_bar(stat = "identity", color = "azure3") + 
       scale_fill_gradient2(name = "Sentiment Average", limits = c(-1,1), low = "red", mid = "white", high = "green", midpoint = 0) +
       ggtitle("Tweets senitment by Location") + 
       ylab("Tweet Count") +
       theme(axis.title.x = element_blank() ,axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)   )
ggarrange(fig1, fig2,fig3, nrow = 3, heights = c(1, 0.25,1))
```
```{r,echo=False}
summary.tibble <- tweet.tibble[tweet.tibble$location == "CA",] %>% group_by(week) %>% summarize(mean_sentiment = mean(sentiment), sd_sentiment = sd(sentiment), count = length(datetime), divisiveness = divisiveness_score(sentiment))
summary.tibble$divisiveness[is.na(summary.tibble$divisiveness)] <- 0

summary.tibble %>% ungroup()

fig1 <- ggplot(summary.tibble, aes(x = week, y = mean_sentiment, fill = mean_sentiment)) + 
      #geom_bar(stat = "identity", color = "azure3") + 
      geom_line(color = "azure3",aes(y=mean_sentiment))+
      geom_point(stat = "identity",color = "azure3",aes(y=mean_sentiment))+
      scale_fill_gradient2(name = "Sentiment Average", limits = c(-1,1), low = "red", mid = "white", high = "green", midpoint = 0) +
      ggtitle("Tweets by Location") + 
      ylab("Tweet Count") +
      theme(axis.title.x = element_blank() ,axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)   )
ggarrange(fig1, nrow = 1, heights = c(1.25))
```

### DataStracture_Checking:
```{r,echo=FALSE}
```

### New_StateMap:
```{r,echo=FALSE}
find.State.Map <- function(row){
  usrPL <- row
  abb <- state.abb 
  name <- state.name 
  
  for(idx in 1:length(abb))
  {
    itr = name[idx]
    if(tolower(itr) == row ){
      return(abb[idx])
    }
  }
  return("USA")
  }

library(maps)
library(ggplot2)
state_data <- ggplot2::map_data('state')
state_data$loc <- sapply(state_data$region,find.State.Map)
result = merge(state_data,summary.tibble,by.x= "loc",by.y="location")
```





```{r,echo=False}
#ggplot(state_data, aes(x = long, y = lat, group = group)) +
#geom_polygon(fill="lightgray", colour = "white")

ggplot(result, aes(x = long, y = lat, group=group ,fill = mean_sentiment)) + 
      geom_polygon(name = "Sentiment Average",colour="black")+
      scale_fill_gradient2(name = "Sentiment Average", limits = c(-1,1), low = "red", mid = "white", high = "green", midpoint = 0)
```
```{r,echo=False}
library(maps)
library(leaflet)
mapStates = map("state", fill = TRUE, plot = FALSE)
leaflet(data = mapStates) %>% addTiles() %>%
  addPolygons(fillColor = topo.colors(10, alpha = NULL), stroke = FALSE)
```



















